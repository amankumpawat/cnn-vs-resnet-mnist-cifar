{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Residual Networks for Robust Image Classification: A Hands-On Investigation of\n",
        "#           Vanishing Gradient Mitigation Across MNIST and CIFAR-10 Datasets\n",
        "\n",
        "# Language: Python 3 (TensorFlow, Keras, PyTorch)\n",
        "# Author(s): Aman Kumpawat\n",
        "\n",
        "# Description:\n",
        "# This notebook implements a full deep learning pipeline for image classification using two datasets:\n",
        "# MNIST (handwritten digits) and CIFAR-10 (colored object images). It explores various architectures\n",
        "# including improved CNNs, ResNet-18, no-skip ResNet, and baseline models. The goal is to benchmark\n",
        "# model performance, visualize training behaviors, and demonstrate both black-box and optional white-box testing.\n",
        "\n",
        "# Inputs:\n",
        "# - TensorFlow datasets: MNIST and CIFAR-10 loaded via keras.datasets\n",
        "# - Model architectures: Keras Sequential APIs and PyTorch custom classes\n",
        "# - Training parameters and manual configurations\n",
        "\n",
        "# Outputs:\n",
        "# - Model summaries and layer visualizations\n",
        "# - Accuracy/loss curves and confusion matrices\n",
        "# - Class distribution plots and prediction samples\n",
        "# - Performance metrics from black-box testing (precision, recall, F1)\n",
        "\n",
        "# Internal Structure (31 Cells Across 5 Major Sections):\n",
        "\n",
        "# SECTION 1: Setup & Dependencies\n",
        "#   Cell 1: Mount Google Drive\n",
        "#   Cell 2: Install & import packages (TensorFlow, Keras, PyTorch, Matplotlib, etc.)\n",
        "#\n",
        "# SECTION 2: MNIST Dataset (Improved CNN in Keras)\n",
        "#   Cell 3: Load & preprocess MNIST, show class balance\n",
        "#   Cell 4: Define Improved CNN for MNIST\n",
        "#   Cell 5: Train the MNIST model with TensorBoard logging\n",
        "#   Cell 6: Evaluate test performance and show confusion matrix\n",
        "#   Cell 7: Visualize predictions and prediction probabilities\n",
        "#   Cell 8: Visualize Conv layer filters and Dense output weights\n",
        "#\n",
        "# SECTION 3: CIFAR-10 Dataset (Improved CNN in Keras)\n",
        "#   Cell 9: Load & normalize CIFAR-10, plot class distribution\n",
        "#   Cell 10: Build deeper CIFAR-10 CNN with regularization\n",
        "#   Cell 11: Train CIFAR-10 CNN, apply class weights\n",
        "#   Cell 12: Plot accuracy/loss over epochs\n",
        "#   Cell 13: Plot log-scale loss\n",
        "#   Cell 14: Evaluate test performance and confusion matrix\n",
        "#   Cell 15: Show CIFAR-10 predictions + prediction bars\n",
        "#   Cell 16: Visualize Conv and Dense weights (CIFAR)\n",
        "#\n",
        "# SECTION 4: ResNet Experiments (PyTorch)\n",
        "#   Cell 17: Define ResidualBlock, ResNet18, NoSkipResNet, BaselineCNN\n",
        "#   Cell 18: Define training loop, optimizer, loss functions\n",
        "#   Cell 18.5: Training Utility Function for ResNet18\n",
        "#   Cell 19: Train ResNet-18 on MNIST\n",
        "#   Cell 20: Plot MNIST ResNet accuracy/loss (log scale)\n",
        "#   Cell 21: Evaluate MNIST ResNet (confusion matrix, predictions)\n",
        "#   Cell 22: Train ResNet-18 on CIFAR-10\n",
        "#   Cell 23: Plot CIFAR-10 ResNet training curves\n",
        "#   Cell 24: Evaluate CIFAR-10 ResNet\n",
        "#   Cell 25: Train NoSkip ResNet on CIFAR\n",
        "#   Cell 26: NoSkip training curves\n",
        "#   Cell 27: Train Baseline CNN on CIFAR\n",
        "#   Cell 28: Evaluate Baseline CNN\n",
        "#\n",
        "# SECTION 5: Final Summary & Testing\n",
        "#   Cell 29: Display model summaries from PyTorch (torchsummary)\n",
        "#   Cell 30: Visualize PyTorch model weights\n",
        "#   Cell 31: Explicit black-box testing using sklearn classification report\n",
        "#\n",
        "# Notes:\n",
        "# - All visualizations follow a clean and consistent matplotlib style.\n",
        "# - TensorBoard is used to track training logs.\n",
        "# - All testing is done with black-box evaluation; white-box is optional and present in weight plots.\n"
      ],
      "metadata": {
        "id": "KTdmKZE6zVQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SECTION 1"
      ],
      "metadata": {
        "id": "vxtZ_5kF189d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Awu1zTwq1-Wj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Install and Import Required Libraries\n",
        "\n",
        "# Description:\n",
        "#   This cell installs and imports all necessary libraries used throughout the deep learning project.\n",
        "#   It ensures the environment is prepared with required packages for model development, training,\n",
        "#   evaluation, and visualization.\n",
        "#   Input: None\n",
        "#   Output: Installs and loads all necessary libraries.\n",
        "\n",
        "# This cell ensures all necessary libraries for deep learning, data preprocessing, evaluation, and visualization are available\n",
        "# in the environment.\n",
        "# It includes both core packages (TensorFlow, Keras, NumPy, Matplotlib) and supporting modules for metrics, model summaries,\n",
        "# and confusion matrices.\n",
        "\n",
        "!pip install -q tensorflow numpy matplotlib seaborn scikit-learn tensorflow-addons --no-deps\n",
        "\n",
        "# import core libraries for deep learning and data analysis\n",
        "import numpy as np                             # numerical computations\n",
        "import matplotlib.pyplot as plt                # plotting and visualizations\n",
        "import seaborn as sns                          # statistical data visualizations\n",
        "import tensorflow as tf                        # core deep learning framework\n",
        "from tensorflow import keras                   # high-level TensorFlow API\n",
        "from tensorflow.keras import layers, models    # neural network components and model building\n",
        "from sklearn.metrics import confusion_matrix   # evaluation metric\n",
        "from tensorflow.keras.utils import to_categorical  # one-hot encoding utility\n"
      ],
      "metadata": {
        "id": "EtxbitRsqCgh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SECTION 2"
      ],
      "metadata": {
        "id": "LBpWgAkX0-8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Load and Preprocess MNIST Dataset\n",
        "\n",
        "# Description:\n",
        "#   This cell loads and preprocesses the MNIST dataset.\n",
        "#   It performs normalization of pixel values, reshapes input for CNNs, computes class distributions,\n",
        "#   visualizes class frequencies, and calculates class weights to compensate for imbalance.\n",
        "#   Input: None\n",
        "#   Output: Preprocessed datasets (x_train, x_test), label arrays (y_train, y_test), class distribution plot,\n",
        "#   class weights dictionary.\n",
        "\n",
        "# This cell loads the MNIST digit dataset, normalizes the pixel data, computes class distributions,\n",
        "# and prepares class weights for handling imbalance (if needed).\n",
        "\n",
        "# load mnist handwritten digits dataset from keras\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "# convert pixel values from 0–255 to 0–1 range for better training stability\n",
        "x_train = x_train.astype(\"float32\") / 255.0\n",
        "x_test = x_test.astype(\"float32\") / 255.0\n",
        "\n",
        "# reshape input data from (samples, 28, 28) to (samples, 28, 28, 1)\n",
        "# required format for Conv2D layers which expect channels last (grayscale: 1 channel)\n",
        "x_train = x_train.reshape(-1, 28, 28, 1)\n",
        "x_test = x_test.reshape(-1, 28, 28, 1)\n",
        "\n",
        "# print basic dataset info: number of samples and image shapes\n",
        "print(f\"MNIST training samples: {x_train.shape[0]}, test samples: {x_test.shape[0]}\")\n",
        "print(f\"MNIST image shape: {x_train.shape[1:]}, Label shape: {y_train.shape}\")\n",
        "\n",
        "# compute distribution of classes in training set\n",
        "# returns unique labels and how often each occurs\n",
        "unique, counts = np.unique(y_train, return_counts=True)\n",
        "class_counts = dict(zip(unique, counts))  # map label to count\n",
        "\n",
        "# display formatted class distribution\n",
        "print(\"\\nMNIST Class Distribution:\")\n",
        "for digit in range(10):\n",
        "    print(f\" - digit {digit}: {class_counts[digit]}\")\n",
        "\n",
        "# plot the frequency of each digit class using a bar chart\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.barplot(x=list(class_counts.keys()), y=list(class_counts.values()), color='skyblue')\n",
        "plt.title(\"MNIST Training Dataset Class Distribution\")\n",
        "plt.xlabel(\"Digit Class\")\n",
        "plt.ylabel(\"Number of Examples\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# compute class weights to balance loss during training\n",
        "# useful when some digits are underrepresented\n",
        "total_samples = len(y_train)\n",
        "print(\"\\nComputed class weights for MNIST:\")\n",
        "class_weights = {}\n",
        "for cls, count in class_counts.items():\n",
        "    # formula: total_samples / (num_classes * class_count)\n",
        "    weight = total_samples / (len(class_counts) * count)\n",
        "    class_weights[cls] = weight\n",
        "    print(f\" - digit {cls}: weight = {weight:.4f}\")\n"
      ],
      "metadata": {
        "id": "-SY3v_7sqITI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Define Improved CNN Model for MNIST\n",
        "\n",
        "# Description:\n",
        "#   This cell defines an improved CNN model architecture for the MNIST dataset.\n",
        "#   The model includes two convolutional blocks with batch normalization and dropout layers for regularization.\n",
        "#   A final dense layer maps features to digit classes.\n",
        "#   Input: None (definition only)\n",
        "#   Output: A compiled Keras Sequential model object ready for training.\n",
        "\n",
        "# This cell defines a deeper CNN model with batch normalization and dropout for better generalization on MNIST.\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout, BatchNormalization\n",
        "\n",
        "# Function Header\n",
        "# Description:\n",
        "#   Constructs and compiles a convolutional neural network for classifying MNIST digit images.\n",
        "#   Parameters: None\n",
        "#   Returns: A compiled Sequential model with convolutional, batch norm, dropout, and dense layers.\n",
        "def build_mnist_model():\n",
        "    model = Sequential(name=\"MNIST_CNN_Improved\")\n",
        "\n",
        "    # Block 1: Two convolutional layers followed by max pooling and dropout\n",
        "    model.add(Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(28, 28, 1)))  # input conv\n",
        "    model.add(BatchNormalization())  # normalize activations to improve training stability\n",
        "    model.add(Conv2D(32, (3, 3), padding='same', activation='relu'))  # second conv layer\n",
        "    model.add(BatchNormalization())  # normalize again\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))  # downsample spatial size\n",
        "    model.add(Dropout(0.25))  # prevent overfitting\n",
        "\n",
        "    # Block 2: Deeper convolutional stack with same structure\n",
        "    model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(0.25))\n",
        "\n",
        "    # Fully connected layers\n",
        "    model.add(Flatten())  # flatten feature maps to 1D vector\n",
        "    model.add(Dense(128, activation='relu'))  # dense hidden layer\n",
        "    model.add(Dropout(0.5))  # heavy dropout for regularization\n",
        "    model.add(Dense(10, activation='softmax'))  # output layer for 10 digit classes\n",
        "\n",
        "    # compile model with Adam optimizer and sparse categorical loss for integer labels\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Object Header\n",
        "# Purpose: Create an instance of the improved CNN model for MNIST classification\n",
        "mnist_model = build_mnist_model()\n",
        "\n",
        "# display architecture summary in the output cell\n",
        "mnist_model.summary()\n"
      ],
      "metadata": {
        "id": "brlMyE6CqU8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Train MNIST_CNN_Improved Model\n",
        "\n",
        "# Description:\n",
        "#   This cell trains the improved CNN model (MNIST_CNN_Improved) on the MNIST dataset.\n",
        "#   Training uses class weights to handle class imbalance and a validation split to monitor performance.\n",
        "#   A TensorBoard callback is included for logging training metrics.\n",
        "#   Input: Preprocessed training data, compiled model\n",
        "#   Output: Training history object (loss, accuracy, val_loss, val_accuracy over epochs)\n",
        "\n",
        "# This cell trains the improved CNN on MNIST using computed class weights to balance the training process.\n",
        "# A validation split is used to monitor overfitting, and TensorBoard callback is added optionally.\n",
        "\n",
        "# training parameters\n",
        "epochs_mnist = 50             # number of training epochs\n",
        "batch_size_mnist = 128        # batch size for training iterations\n",
        "\n",
        "# Object Header\n",
        "# Purpose: TensorBoard callback to enable real-time visualization of training metrics\n",
        "# Note: Viewable via `%tensorboard --logdir logs/mnist` in a notebook environment\n",
        "tensorboard_cb = keras.callbacks.TensorBoard(log_dir=\"./logs/mnist\", histogram_freq=1)\n",
        "\n",
        "# Object Header\n",
        "# Purpose: Train the CNN model on training data using weighted classes and validation split\n",
        "# Returns: Keras History object that stores loss and accuracy per epoch\n",
        "history_mnist = mnist_model.fit(\n",
        "    x_train, y_train,\n",
        "    validation_split=0.1,        # 10% of training data used for validation\n",
        "    batch_size=batch_size_mnist,\n",
        "    epochs=epochs_mnist,\n",
        "    class_weight=class_weights,  # use previously computed weights to balance loss\n",
        "    callbacks=[tensorboard_cb],  # enable logging callback\n",
        "    verbose=2                    # show training progress per epoch\n",
        ")\n"
      ],
      "metadata": {
        "id": "aPom7C6oqnfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Evaluate MNIST Model Performance (Accuracy, Loss, Confusion Matrix)\n",
        "\n",
        "# Description:\n",
        "#   This cell evaluates the performance of the trained CNN model on the MNIST test dataset.\n",
        "#   It uses black-box testing to report test accuracy and loss, and visualizes model predictions\n",
        "#   using a confusion matrix heatmap to show classification accuracy per digit class.\n",
        "#   Input: Trained model, test data (x_test, y_test)\n",
        "#   Output: Accuracy and loss metrics, confusion matrix plot\n",
        "\n",
        "# This cell evaluates the trained CNN on the MNIST test set using black-box testing.\n",
        "# It computes accuracy, loss, and shows a heatmap of the confusion matrix for visualizing prediction\n",
        "# performance across classes.\n",
        "\n",
        "# evaluate model performance on test set\n",
        "test_loss, test_acc = mnist_model.evaluate(x_test, y_test, verbose=0)\n",
        "print(f\"MNIST Test Accuracy: {test_acc:.4f}\")\n",
        "print(f\"MNIST Test Loss: {test_loss:.4f}\")\n",
        "\n",
        "# generate class probability predictions for test set\n",
        "y_pred_probs = mnist_model.predict(x_test, verbose=0)\n",
        "\n",
        "# select most likely predicted class for each example\n",
        "y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "# compute confusion matrix comparing predicted and true labels\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# visualize confusion matrix as annotated heatmap\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=True)\n",
        "plt.title(\"MNIST Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.xticks(ticks=np.arange(10), labels=np.arange(10))\n",
        "plt.yticks(ticks=np.arange(10), labels=np.arange(10))\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xWIrz6aWqsgC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Visualize MNIST Predictions and Class Probabilities\n",
        "\n",
        "# Description:\n",
        "#   This cell visualizes model predictions on sample MNIST test images.\n",
        "#   Each image is shown alongside its predicted label and a softmax probability bar chart.\n",
        "#   This visual illustrates model confidence and prediction correctness across digit classes.\n",
        "#   Input: Trained model, sample subset of x_test/y_test\n",
        "#   Output: Grid of digit images and confidence bar plots per sample\n",
        "\n",
        "# This cell shows a grid of sample predictions alongside their true labels, and plots softmax probability\n",
        "# bars for each image.\n",
        "# It highlights the model’s confidence, correctness, and class-level distinctions (499testing3-style).\n",
        "\n",
        "# number of samples to visualize\n",
        "num_images = 8\n",
        "\n",
        "# select first `num_images` from the test set\n",
        "sample_images = x_test[:num_images]\n",
        "sample_labels = y_test[:num_images]\n",
        "\n",
        "# predict class probabilities for selected samples\n",
        "sample_probs = mnist_model.predict(sample_images, verbose=0)\n",
        "\n",
        "# convert probabilities to predicted class indices\n",
        "sample_preds = np.argmax(sample_probs, axis=1)\n",
        "\n",
        "# create subplots: top row for images, bottom row for confidence bars\n",
        "fig, axes = plt.subplots(2, num_images, figsize=(16, 5))\n",
        "fig.suptitle(\"MNIST Predictions and Class Probabilities\", fontsize=16)\n",
        "\n",
        "for i in range(num_images):\n",
        "    # plot grayscale digit image with true and predicted label\n",
        "    axes[0, i].imshow(sample_images[i].squeeze(), cmap='gray')\n",
        "    axes[0, i].set_title(\n",
        "        f\"True: {sample_labels[i]}\\nPred: {sample_preds[i]}\",\n",
        "        fontsize=10,\n",
        "        color='green' if sample_preds[i] == sample_labels[i] else 'red'  # highlight correct/incorrect predictions\n",
        "    )\n",
        "    axes[0, i].axis('off')\n",
        "\n",
        "    # plot softmax probability distribution across all 10 digit classes\n",
        "    axes[1, i].bar(np.arange(10), sample_probs[i], color='gray')\n",
        "    axes[1, i].set_xticks(np.arange(10))\n",
        "    axes[1, i].set_ylim(0, 1)\n",
        "    axes[1, i].set_title(\"Confidence\", fontsize=10)\n",
        "    axes[1, i].set_xlabel(\"Digit Class\")\n",
        "\n",
        "# ensure proper spacing between plots\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "6hA2RAPoqyDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8: Visualize Conv and Dense Weights (MNIST)\n",
        "\n",
        "# Description:\n",
        "#   This cell provides white-box testing visualizations of the MNIST model.\n",
        "#   It displays the learned convolutional filters from the first Conv2D layer\n",
        "#   and the weights from the final dense (output) layer as a heatmap.\n",
        "#   Input: Trained CNN model\n",
        "#   Output: Visualizations of conv filters and dense layer weights\n",
        "\n",
        "# This cell visualizes the learned filters from the first convolutional layer, and the dense layer weights as a heatmap.\n",
        "# It helps evaluate internal structure and feature separation (white-box testing).\n",
        "\n",
        "# Function Header\n",
        "# Description:\n",
        "#   Extracts and visualizes filters from the first Conv2D layer in the model.\n",
        "#   Parameters: model - trained Keras model, dataset_name - label for the plot title\n",
        "#   Returns: None (displays grayscale filter images)\n",
        "def show_conv1_weights(model, dataset_name=\"MNIST\"):\n",
        "    conv_weights = model.layers[0].get_weights()[0]  # shape: (3, 3, 1, 32)\n",
        "    num_filters = conv_weights.shape[-1]\n",
        "    fig, axes = plt.subplots(1, min(8, num_filters), figsize=(12, 3))\n",
        "    fig.suptitle(f\"{dataset_name} Conv Layer 1 Filters\", fontsize=14)\n",
        "\n",
        "    for i in range(min(8, num_filters)):\n",
        "        filt = conv_weights[:, :, 0, i]  # extract filter i\n",
        "        axes[i].imshow(filt, cmap='gray')\n",
        "        axes[i].axis(\"off\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Function Header\n",
        "# Description:\n",
        "#   Extracts and visualizes weights from the final Dense output layer.\n",
        "#   Parameters: model - trained Keras model, dataset_name - label for the plot title\n",
        "#   Returns: None (displays a heatmap of weights)\n",
        "def show_dense_weights(model, dataset_name=\"MNIST\"):\n",
        "    dense_weights = None\n",
        "    for layer in model.layers:\n",
        "        if isinstance(layer, Dense) and layer.units == 10:  # match output layer with 10 classes\n",
        "            dense_weights = layer.get_weights()[0]  # extract weights matrix\n",
        "            break\n",
        "\n",
        "    if dense_weights is not None:\n",
        "        plt.figure(figsize=(10, 4))\n",
        "        sns.heatmap(dense_weights.T, cmap='magma', cbar=True)\n",
        "        plt.title(f\"{dataset_name} Dense Layer Weight Matrix (Output Layer)\")\n",
        "        plt.xlabel(\"Features\")\n",
        "        plt.ylabel(\"Digit Classes\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"Dense output layer not found.\")\n",
        "\n",
        "# Object Header\n",
        "# Purpose: Call white-box testing functions to visualize CNN internals\n",
        "show_conv1_weights(mnist_model)\n",
        "show_dense_weights(mnist_model)\n"
      ],
      "metadata": {
        "id": "j5EX06TNq3BC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SECTION 3"
      ],
      "metadata": {
        "id": "0rh9ZE_907Ct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9: Load and Preprocess CIFAR-10 Dataset\n",
        "\n",
        "# Description:\n",
        "#   This cell loads and preprocesses the CIFAR-10 dataset.\n",
        "#   It includes normalization of pixel values, flattening label arrays, class name mapping,\n",
        "#   visualization of class distribution, and computation of class weights for balanced training.\n",
        "#   Input: None\n",
        "#   Output: Preprocessed CIFAR-10 training/test sets, class weights dictionary, class distribution plot\n",
        "\n",
        "# This cell loads the CIFAR-10 dataset, normalizes the image data, maps class labels to names,\n",
        "# and computes class weights for balanced training.\n",
        "\n",
        "import pandas as pd  # fix: import pandas\n",
        "\n",
        "# load cifar-10 data from keras\n",
        "(x_train_cifar, y_train_cifar), (x_test_cifar, y_test_cifar) = keras.datasets.cifar10.load_data()\n",
        "y_train_cifar = y_train_cifar.flatten()  # flatten label arrays to 1D\n",
        "y_test_cifar = y_test_cifar.flatten()\n",
        "\n",
        "# normalize pixel values to range [0,1] for stable training\n",
        "x_train_cifar = x_train_cifar.astype(\"float32\") / 255.0\n",
        "x_test_cifar = x_test_cifar.astype(\"float32\") / 255.0\n",
        "\n",
        "# print dataset shape information\n",
        "print(f\"CIFAR-10 training samples: {x_train_cifar.shape[0]}, test samples: {x_test_cifar.shape[0]}\")\n",
        "print(f\"Image shape: {x_train_cifar.shape[1:]}, Label shape: {y_train_cifar.shape}\")\n",
        "\n",
        "# class labels for CIFAR-10\n",
        "cifar10_classes = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "                   'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "# compute number of examples per class\n",
        "cifar_counts = pd.Series(y_train_cifar).value_counts().sort_index()\n",
        "\n",
        "# print label counts alongside class names\n",
        "print(\"\\nCIFAR-10 Class Distribution:\")\n",
        "for i, count in cifar_counts.items():\n",
        "    print(f\" - {cifar10_classes[i]} ({i}): {count}\")\n",
        "\n",
        "# visualize class distribution with bar plot\n",
        "plt.figure(figsize=(7, 4))\n",
        "sns.barplot(x=cifar10_classes, y=cifar_counts.values, hue=cifar10_classes, legend=False, palette='Greens')\n",
        "plt.title(\"CIFAR-10 Training Dataset Class Distribution\")\n",
        "plt.xlabel(\"Class\")\n",
        "plt.ylabel(\"Number of Samples\")\n",
        "plt.xticks(rotation=30)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# compute class weights using sklearn utility\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "cifar_class_weights = class_weight.compute_class_weight(\n",
        "    class_weight='balanced',               # use inverse frequency formula\n",
        "    classes=np.unique(y_train_cifar),      # class labels\n",
        "    y=y_train_cifar                        # target labels\n",
        ")\n",
        "cifar_class_weights = dict(enumerate(cifar_class_weights))  # convert to dictionary for training API\n",
        "\n",
        "# print computed weights for each class\n",
        "print(\"\\nComputed class weights for CIFAR-10:\")\n",
        "for k, v in cifar_class_weights.items():\n",
        "    print(f\" - {cifar10_classes[k]} ({k}): {v:.4f}\")\n"
      ],
      "metadata": {
        "id": "-oZOQSMRq-5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 10: Build CIFAR10_CNN_Improved Model\n",
        "\n",
        "# Description:\n",
        "#   This cell defines and compiles an improved CNN model for the CIFAR-10 dataset.\n",
        "#   The architecture consists of three convolutional blocks with increasing filter sizes,\n",
        "#   each followed by batch normalization and dropout. Fully connected layers follow for final classification.\n",
        "#   Input: None (model structure defined internally)\n",
        "#   Output: A compiled Keras Sequential model object for CIFAR-10 classification\n",
        "\n",
        "# This cell defines a deeper CNN model for CIFAR-10 with 3 convolutional blocks using\n",
        "# increasing filter sizes, batch normalization, dropout, and two fully connected layers.\n",
        "\n",
        "from tensorflow.keras import models, layers\n",
        "\n",
        "# Function Header\n",
        "# Description:\n",
        "#   Builds and compiles a CNN for classifying CIFAR-10 images.\n",
        "#   The model uses Conv2D, BatchNormalization, Dropout, MaxPooling, and Dense layers.\n",
        "#   Returns: Compiled Keras model object\n",
        "def build_cifar_model():\n",
        "    model = models.Sequential(name=\"CIFAR10_CNN_Improved\")\n",
        "\n",
        "    # Block 1: Initial feature extraction with small filters\n",
        "    model.add(layers.Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(32, 32, 3)))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Conv2D(32, (3, 3), padding='same', activation='relu'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))  # downsample spatial resolution\n",
        "    model.add(layers.Dropout(0.25))  # regularization\n",
        "\n",
        "    # Block 2: Deeper feature extraction\n",
        "    model.add(layers.Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    # Block 3: Higher-level patterns\n",
        "    model.add(layers.Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(layers.Dropout(0.4))\n",
        "\n",
        "    # Fully connected classifier\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(256, activation='relu'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Dropout(0.5))\n",
        "    model.add(layers.Dense(10, activation='softmax'))  # final output layer for 10 CIFAR classes\n",
        "\n",
        "    # Compile model with categorical loss and accuracy metric\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Object Header\n",
        "# Purpose: Instantiate and summarize the CIFAR-10 CNN model\n",
        "cifar_model = build_cifar_model()\n",
        "cifar_model.summary()\n"
      ],
      "metadata": {
        "id": "LFlN63LqrE9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 11: Train CIFAR-10 CNN Improved Model\n",
        "\n",
        "# Description:\n",
        "#   This cell trains the improved CNN model for CIFAR-10.\n",
        "#   It uses class weights to balance training, a validation split to monitor overfitting,\n",
        "#   and TensorBoard for logging accuracy/loss during training.\n",
        "#   Input: Preprocessed CIFAR-10 data, compiled model\n",
        "#   Output: Training history object with accuracy/loss metrics\n",
        "\n",
        "# This cell trains the improved CIFAR-10 model using the training data, with validation split\n",
        "# and class weights applied.\n",
        "# A clean training log is shown using TensorBoard logging for visualization of performance curves.\n",
        "\n",
        "# set training parameters\n",
        "epochs_cifar = 50             # number of training epochs\n",
        "batch_size_cifar = 128        # batch size per training iteration\n",
        "\n",
        "# Object Header\n",
        "# Purpose: TensorBoard callback for CIFAR-10 training visualization\n",
        "# Output: Logs training and validation metrics to ./logs/cifar10_improved\n",
        "tensorboard_cb_cifar = keras.callbacks.TensorBoard(\n",
        "    log_dir=\"./logs/cifar10_improved\",\n",
        "    histogram_freq=1\n",
        ")\n",
        "\n",
        "# Object Header\n",
        "# Purpose: Train the compiled CIFAR-10 CNN model with validation and class balancing\n",
        "# Returns: Keras History object containing training/validation loss and accuracy\n",
        "history_cifar = cifar_model.fit(\n",
        "    x_train_cifar, y_train_cifar,\n",
        "    epochs=epochs_cifar,\n",
        "    batch_size=batch_size_cifar,\n",
        "    validation_split=0.1,             # hold out 10% of training data for validation\n",
        "    class_weight=cifar_class_weights, # apply computed weights to account for class imbalance\n",
        "    callbacks=[tensorboard_cb_cifar], # enable TensorBoard logging\n",
        "    verbose=2\n",
        ")\n"
      ],
      "metadata": {
        "id": "0fV7EHgGrMM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 12: Plot CIFAR-10 Training and Validation Curves\n",
        "\n",
        "# Description:\n",
        "#   This cell visualizes training and validation accuracy/loss over epochs for the CIFAR-10 model.\n",
        "#   It plots both curves to assess learning trends, convergence, and overfitting.\n",
        "#   Input: Keras training history object\n",
        "#   Output: Accuracy and loss plots for training vs. validation sets\n",
        "\n",
        "# Cell 12: Plot CIFAR-10 Training and Validation Curves\n",
        "# This cell visualizes training accuracy and loss per epoch alongside validation metrics.\n",
        "# It helps track convergence and potential overfitting (mirrors MNIST plotting structure).\n",
        "\n",
        "# extract accuracy and loss from training history\n",
        "acc = history_cifar.history['accuracy']         # training accuracy per epoch\n",
        "val_acc = history_cifar.history['val_accuracy'] # validation accuracy per epoch\n",
        "loss = history_cifar.history['loss']            # training loss per epoch\n",
        "val_loss = history_cifar.history['val_loss']    # validation loss per epoch\n",
        "epochs_range = range(1, len(acc) + 1)           # x-axis range for plots\n",
        "\n",
        "# plot training vs. validation accuracy\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.title(\"CIFAR-10 Accuracy per Epoch\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# plot training vs. validation loss\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.title(\"CIFAR-10 Loss per Epoch\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "rw_vobRerUI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 13: Log-Scale Loss Plot for CIFAR-10\n",
        "\n",
        "# Description:\n",
        "#   This cell visualizes training and validation loss curves on a logarithmic scale for the CIFAR-10 model.\n",
        "#   Log-scaling makes it easier to interpret trends in loss values, especially when they become small.\n",
        "#   Input: Training and validation loss arrays\n",
        "#   Output: Log-scaled loss plot per epoch\n",
        "\n",
        "# This cell plots the training and validation loss curves using a logarithmic scale on the y-axis.\n",
        "# It helps visualize performance trends more clearly when loss values become small.\n",
        "\n",
        "# log-scale loss plot\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(epochs_range, loss, label='Training Loss')         # plot raw training loss\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')   # plot raw validation loss\n",
        "plt.yscale('log')                                            # apply log scaling to y-axis\n",
        "plt.title(\"CIFAR-10 Log-Scale Loss per Epoch\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Log Loss\")\n",
        "plt.legend()\n",
        "plt.grid(True, which=\"both\", ls=\"--\", linewidth=0.5)         # enable fine grid lines\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Xzx1zrrMrbLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 14: Evaluate Test Accuracy and Confusion Matrix (CIFAR-10)\n",
        "\n",
        "# Description:\n",
        "#   This cell evaluates the final performance of the trained CIFAR-10 model on the test set.\n",
        "#   It prints test accuracy and loss (black-box testing), then visualizes prediction correctness\n",
        "#   using a labeled confusion matrix.\n",
        "#   Input: Trained model and CIFAR-10 test set\n",
        "#   Output: Accuracy/loss metrics and confusion matrix heatmap\n",
        "\n",
        "# This cell evaluates the trained CIFAR-10 model using the test set, prints the test accuracy and loss,\n",
        "# and visualizes the confusion matrix to show true vs predicted class distributions.\n",
        "\n",
        "# evaluate model on test data\n",
        "test_loss, test_acc = cifar_model.evaluate(x_test_cifar, y_test_cifar, verbose=0)\n",
        "print(f\"CIFAR-10 Test Accuracy: {test_acc:.4f}, Test Loss: {test_loss:.4f}\")\n",
        "\n",
        "# generate prediction probabilities for all test samples\n",
        "y_pred_probs = cifar_model.predict(x_test_cifar, verbose=0)\n",
        "\n",
        "# convert probabilities to class indices\n",
        "y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "# compute confusion matrix using true vs predicted labels\n",
        "conf_mat = confusion_matrix(y_test_cifar, y_pred)\n",
        "\n",
        "# plot confusion matrix with class name annotations\n",
        "plt.figure(figsize=(7, 6))\n",
        "sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=cifar10_classes,\n",
        "            yticklabels=cifar10_classes)\n",
        "plt.title(\"CIFAR-10 Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.xticks(rotation=30)\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "2QJvkCVGrfeG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 15: Show CIFAR-10 Predictions and Probability Bars\n",
        "\n",
        "# Description:\n",
        "#   This cell performs black-box testing visualization for the CIFAR-10 model.\n",
        "#   It randomly selects test images, shows predicted vs true labels, and\n",
        "#   plots softmax probability distributions to interpret model confidence.\n",
        "#   Input: Trained model, CIFAR-10 test data\n",
        "#   Output: Grid of test images with class probability bar plots\n",
        "\n",
        "# This cell displays a few test images from CIFAR-10 alongside their predicted labels,\n",
        "# true labels, and class probability distributions.\n",
        "\n",
        "# randomly select a subset of test images to visualize\n",
        "num_images = 6\n",
        "indices = np.random.choice(len(x_test_cifar), num_images, replace=False)\n",
        "images = x_test_cifar[indices]             # image samples\n",
        "true_labels = y_test_cifar[indices]        # corresponding true class labels\n",
        "\n",
        "# get predicted probabilities and class labels\n",
        "pred_probs = cifar_model.predict(images)   # softmax probabilities\n",
        "pred_labels = np.argmax(pred_probs, axis=1)  # predicted class indices\n",
        "\n",
        "# create side-by-side subplot layout (image + probability bars)\n",
        "fig, axes = plt.subplots(num_images, 2, figsize=(10, 2.2 * num_images))\n",
        "\n",
        "for i in range(num_images):\n",
        "    # show image with predicted and true class\n",
        "    axes[i, 0].imshow(images[i])\n",
        "    axes[i, 0].axis('off')\n",
        "    axes[i, 0].set_title(\n",
        "        f\"True: {cifar10_classes[true_labels[i]]}\\nPred: {cifar10_classes[pred_labels[i]]}\",\n",
        "        color='green' if true_labels[i] == pred_labels[i] else 'red',\n",
        "        fontsize=11\n",
        "    )\n",
        "\n",
        "    # plot bar chart of class probabilities\n",
        "    axes[i, 1].bar(np.arange(10), pred_probs[i], color='skyblue')\n",
        "    axes[i, 1].set_xticks(np.arange(10))\n",
        "    axes[i, 1].set_xticklabels(cifar10_classes, rotation=30)\n",
        "    axes[i, 1].set_ylim(0, 1)\n",
        "    axes[i, 1].set_title(\"Prediction Probabilities\", fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GSFyJeZUrliu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 16: Visualize Weights of Conv and Dense Layers (CIFAR-10)\n",
        "\n",
        "# Description:\n",
        "#   This cell performs white-box testing visualizations of the trained CIFAR-10 model.\n",
        "#   It displays the learned filters from the first convolutional layer and the weights\n",
        "#   of the final dense layer in heatmap form to inspect internal feature representations.\n",
        "#   Input: Trained model\n",
        "#   Output: Filter visualizations and dense layer heatmap\n",
        "\n",
        "# This cell visualizes the first layer's learned filters and the fully connected (dense) layer weight matrix.\n",
        "\n",
        "# Function Header\n",
        "# Description:\n",
        "#   Displays visualizations of the learned filters from the first convolutional layer of the CIFAR-10 model.\n",
        "#   Parameters: model - trained Keras model\n",
        "#   Returns: None (shows a grid of RGB filters)\n",
        "def show_cifar_conv_filters(model):\n",
        "    filters = model.layers[0].get_weights()[0]  # shape: (3, 3, 3, 32)\n",
        "    filters = (filters - filters.min()) / (filters.max() - filters.min())  # normalize filter values to [0, 1]\n",
        "    num_filters = filters.shape[-1]\n",
        "    n_cols = 8\n",
        "    n_rows = (num_filters + n_cols - 1) // n_cols  # ceiling division to get rows needed\n",
        "\n",
        "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(12, 1.5 * n_rows))\n",
        "    fig.suptitle(\"CIFAR-10 Conv1 Filters\", fontsize=14)\n",
        "\n",
        "    for i in range(n_rows * n_cols):\n",
        "        ax = axes[i // n_cols, i % n_cols]\n",
        "        if i < num_filters:\n",
        "            ax.imshow(filters[:, :, :, i])  # visualize each filter as RGB image\n",
        "        ax.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Function Header\n",
        "# Description:\n",
        "#   Visualizes the learned weight matrix from the final dense layer using a heatmap.\n",
        "#   Parameters: model - trained Keras model\n",
        "#   Returns: None (displays heatmap of weights)\n",
        "def show_cifar_fc_weights(model):\n",
        "    for layer in model.layers[::-1]:\n",
        "        if isinstance(layer, layers.Dense):  # search backward for the last Dense layer\n",
        "            weights = layer.get_weights()[0]\n",
        "            break\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    sns.heatmap(weights, cmap='magma', cbar=True)\n",
        "    plt.title(\"CIFAR-10 Final Dense Layer Weights\")\n",
        "    plt.xlabel(\"Features\")\n",
        "    plt.ylabel(\"Classes\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Object Header\n",
        "# Purpose: Call visualization functions to inspect internal filter and weight structures\n",
        "show_cifar_conv_filters(cifar_model)\n",
        "show_cifar_fc_weights(cifar_model)\n"
      ],
      "metadata": {
        "id": "pC-4NKdmrlgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SECTION 4"
      ],
      "metadata": {
        "id": "Q0lbPMmq01kg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 17: Define ADTs - ResidualBlock, ResNet18, BaselineCNN\n",
        "# This cell defines the core architecture components used in the custom ResNet and baseline CNN models.\n",
        "# These user-defined ADTs include ResidualBlock (with skip connections), a simplified ResNet18, and a basic CNN without skip connections.\n",
        "# These modules allow experimentation with model depth and skip vs no-skip architectures across datasets.\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# user-defined ADT: ResidualBlock (standard skip connection block for ResNet)\n",
        "# description: builds a 2-layer convolutional residual block with optional downsampling\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3,\n",
        "                               stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.skip = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.skip = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1,\n",
        "                          stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.skip(x)\n",
        "        return F.relu(out)\n",
        "\n",
        "# user-defined ADT: ResNet18\n",
        "# description: defines a 4-block ResNet model using stacked residual blocks with global average pooling and a final classifier\n",
        "class ResNet18(nn.Module):\n",
        "    def __init__(self, num_classes=10, in_channels=3):\n",
        "        super(ResNet18, self).__init__()\n",
        "        self.in_channels = 64\n",
        "        self.conv1 = nn.Conv2d(in_channels, 64, kernel_size=3, stride=1,\n",
        "                               padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "\n",
        "        self.layer1 = self._make_layer(64, 2, stride=1)\n",
        "        self.layer2 = self._make_layer(128, 2, stride=2)\n",
        "        self.layer3 = self._make_layer(256, 2, stride=2)\n",
        "        self.layer4 = self._make_layer(512, 2, stride=2)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512, num_classes)\n",
        "\n",
        "    def _make_layer(self, out_channels, blocks, stride):\n",
        "        layers = [ResidualBlock(self.in_channels, out_channels, stride)]\n",
        "        self.in_channels = out_channels\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(ResidualBlock(out_channels, out_channels))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# user-defined ADT: BaselineCNN\n",
        "# description: defines a simple 4-layer convolutional model with downsampling but no skip connections\n",
        "class BaselineCNN(nn.Module):\n",
        "    def __init__(self, num_classes=10, in_channels=3):\n",
        "        super(BaselineCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, 64, kernel_size=3, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(128)\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(256)\n",
        "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, padding=1, bias=False)\n",
        "        self.bn4 = nn.BatchNorm2d(512)\n",
        "        self.pool = nn.AdaptiveAvgPool2d((1,1))\n",
        "        self.fc = nn.Linear(512, num_classes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = self.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = self.relu(self.bn3(self.conv3(x)))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = self.relu(self.bn4(self.conv4(x)))\n",
        "        x = self.pool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "TwaCiiSRfdNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 18: Define Training Loop, Optimizer, and Learning Rate Scheduler\n",
        "\n",
        "# Description:\n",
        "#   This cell defines training and evaluation utilities for PyTorch models using black-box testing methodology.\n",
        "#   It includes label smoothing loss, learning rate scheduling, mixed-precision support, and training loops.\n",
        "#   Input: PyTorch model, dataloaders, optimizer, loss function, epoch count\n",
        "#   Output: Best trained model (by validation accuracy), accuracy and loss per epoch\n",
        "\n",
        "# This cell defines helper functions for training and evaluation of PyTorch models using black-box testing principles.\n",
        "# It includes support for mixed-precision training (optional), accuracy tracking, and adjustable learning rate.\n",
        "\n",
        "import torch.optim as optim\n",
        "import copy\n",
        "import time\n",
        "\n",
        "# User-Defined ADT Header\n",
        "# ADT Name: SmoothCrossEntropyLoss\n",
        "# Description: Implements label smoothing to regularize classification and prevent overconfident predictions.\n",
        "class SmoothCrossEntropyLoss(nn.Module):\n",
        "    def __init__(self, smoothing=0.1):\n",
        "        super(SmoothCrossEntropyLoss, self).__init__()\n",
        "        self.smoothing = smoothing\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        num_classes = pred.size(1)\n",
        "        one_hot = torch.zeros_like(pred).scatter(1, target.unsqueeze(1), 1)\n",
        "        one_hot = one_hot * (1 - self.smoothing) + (1 - one_hot) * self.smoothing / (num_classes - 1)\n",
        "        log_prob = torch.nn.functional.log_softmax(pred, dim=1)\n",
        "        return -(one_hot * log_prob).sum(dim=1).mean()\n",
        "\n",
        "# Function Header\n",
        "# Description:\n",
        "#   Adjusts the learning rate based on epoch number using step decay.\n",
        "#   Decreases learning rate after fixed epoch thresholds.\n",
        "# Parameters: optimizer - optimizer to adjust, epoch - current epoch number, base_lr - starting learning rate\n",
        "# Returns: None\n",
        "def adjust_learning_rate(optimizer, epoch, base_lr=0.1):\n",
        "    lr = base_lr\n",
        "    if epoch > 40:\n",
        "        lr *= 0.01\n",
        "    elif epoch > 20:\n",
        "        lr *= 0.1\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "# Function Header\n",
        "# Description:\n",
        "#   Trains the model for one epoch using mixed-precision if enabled.\n",
        "#   Computes running loss and accuracy across the epoch.\n",
        "# Parameters: model, loader, criterion, optimizer, scaler, epoch, total_epochs\n",
        "# Returns: Average loss, accuracy\n",
        "def train_one_epoch(model, loader, criterion, optimizer, scaler, epoch, total_epochs):\n",
        "    model.train()\n",
        "    correct, total, running_loss = 0, 0, 0.0\n",
        "    start_time = time.time()\n",
        "\n",
        "    for inputs, targets in loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # use mixed-precision context if enabled\n",
        "        with torch.cuda.amp.autocast(enabled=USE_FP16):\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    acc = correct / total\n",
        "    avg_loss = running_loss / total\n",
        "    print(f\"[Epoch {epoch}/{total_epochs}] Train Loss: {avg_loss:.4f} | Train Acc: {acc:.4f} | Time: {time.time() - start_time:.2f}s\")\n",
        "    return avg_loss, acc\n",
        "\n",
        "# Function Header\n",
        "# Description:\n",
        "#   Evaluates model performance on validation/test set.\n",
        "#   Computes average loss and classification accuracy.\n",
        "# Parameters: model, loader, criterion\n",
        "# Returns: Average loss, accuracy\n",
        "def evaluate(model, loader, criterion):\n",
        "    model.eval()\n",
        "    correct, total, running_loss = 0, 0, 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    acc = correct / total\n",
        "    avg_loss = running_loss / total\n",
        "    return avg_loss, acc\n",
        "\n",
        "# Function Header\n",
        "# Description:\n",
        "#   Trains and evaluates a PyTorch model over multiple epochs.\n",
        "#   Tracks and restores the best model based on validation accuracy.\n",
        "# Parameters: model, train_loader, test_loader, epochs, criterion, optimizer\n",
        "# Returns: Best-performing model and its validation accuracy\n",
        "def train_and_evaluate_model(model, train_loader, test_loader, epochs, criterion, optimizer):\n",
        "    model.to(device)\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=USE_FP16)\n",
        "\n",
        "    best_acc = 0.0\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        adjust_learning_rate(optimizer, epoch)\n",
        "        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, scaler, epoch, epochs)\n",
        "        val_loss, val_acc = evaluate(model, test_loader, criterion)\n",
        "        print(f\"Eval Loss: {val_loss:.4f} | Eval Acc: {val_acc:.4f}\\n\")\n",
        "\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, best_acc\n"
      ],
      "metadata": {
        "id": "qx7EO3KCrlaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 18.5: Training Utility Function for ResNet18\n",
        "\n",
        "# Description:\n",
        "#   This function implements a PyTorch training utility for models such as ResNet18.\n",
        "#   It trains and evaluates over a given number of epochs, tracks accuracy and loss for both training\n",
        "#   and validation sets, and returns the best model based on validation accuracy.\n",
        "#   Input: PyTorch model, DataLoaders, epoch count, loss function, optimizer\n",
        "#   Output: Trained model, best validation accuracy, and full training/validation performance history\n",
        "\n",
        "# This function handles training, evaluation, and tracks loss/accuracy across epochs (PyTorch).\n",
        "\n",
        "# Function Header\n",
        "# Description:\n",
        "#   Executes the training and evaluation process over multiple epochs for a given PyTorch model.\n",
        "#   Tracks and prints epoch-wise performance, storing the best validation accuracy.\n",
        "# Parameters:\n",
        "#   - model: a PyTorch model (e.g., ResNet18)\n",
        "#   - train_loader: DataLoader for training data\n",
        "#   - test_loader: DataLoader for validation/testing\n",
        "#   - epochs: number of training epochs\n",
        "#   - criterion: loss function\n",
        "#   - optimizer: optimizer object (e.g., SGD or Adam)\n",
        "# Returns:\n",
        "#   - model: trained model with best weights\n",
        "#   - best_acc: highest validation accuracy achieved\n",
        "#   - train_acc, val_acc: lists of accuracy per epoch\n",
        "#   - train_loss, val_loss: lists of loss per epoch\n",
        "def train_and_evaluate_model(model, train_loader, test_loader, epochs, criterion, optimizer):\n",
        "    train_acc, val_acc = [], []\n",
        "    train_loss, val_loss = []\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        running_loss = 0.0\n",
        "\n",
        "        # training loop for one epoch\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "        train_acc.append(correct / total)\n",
        "        train_loss.append(running_loss / len(train_loader))\n",
        "\n",
        "        # evaluation loop\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        val_running_loss = 0.0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, labels in test_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_running_loss += loss.item()\n",
        "                _, predicted = outputs.max(1)\n",
        "                correct += predicted.eq(labels).sum().item()\n",
        "                total += labels.size(0)\n",
        "\n",
        "        val_acc_epoch = correct / total\n",
        "        val_acc.append(val_acc_epoch)\n",
        "        val_loss.append(val_running_loss / len(test_loader))\n",
        "\n",
        "        if val_acc_epoch > best_acc:\n",
        "            best_acc = val_acc_epoch\n",
        "\n",
        "        print(f\"Epoch {epoch}/{epochs} - Train Acc: {train_acc[-1]:.4f}, Val Acc: {val_acc[-1]:.4f}, Train Loss: {train_loss[-1]:.4f}, Val Loss: {val_loss[-1]:.4f}\")\n",
        "\n",
        "    return model, best_acc, train_acc, val_acc, train_loss, val_loss\n"
      ],
      "metadata": {
        "id": "Ro1Cf4ykrlX7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 19: Train ResNet-18 on MNIST\n",
        "\n",
        "# Description:\n",
        "#   This cell trains a ResNet-18 model on the MNIST dataset using PyTorch.\n",
        "#   It uses label smoothing for regularization, SGD optimizer with momentum,\n",
        "#   and tracks performance via black-box testing (accuracy/loss on test set).\n",
        "#   Input: MNIST data (converted to PyTorch tensors)\n",
        "#   Output: Trained model and printed best test accuracy\n",
        "\n",
        "# This cell initializes and trains a ResNet-18 model on the MNIST dataset using PyTorch.\n",
        "# It uses label smoothing, SGD optimizer, and black-box testing metrics to track best validation accuracy.\n",
        "\n",
        "# setup: training hyperparameters\n",
        "EPOCHS_MNIST_RESNET = 50\n",
        "LEARNING_RATE = 0.1\n",
        "MOMENTUM = 0.9\n",
        "WEIGHT_DECAY = 5e-4\n",
        "LABEL_SMOOTHING = 0.1\n",
        "USE_FP16 = True  # enable mixed-precision training\n",
        "\n",
        "# Object Header\n",
        "# Purpose: Select appropriate hardware (GPU if available) for model training\n",
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# reshape MNIST dataset to match PyTorch (N, C, H, W) input format\n",
        "x_train_torch = torch.tensor(x_train.reshape(-1, 1, 28, 28), dtype=torch.float32)\n",
        "y_train_torch = torch.tensor(y_train, dtype=torch.long)\n",
        "x_test_torch = torch.tensor(x_test.reshape(-1, 1, 28, 28), dtype=torch.float32)\n",
        "y_test_torch = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# Object Header\n",
        "# Purpose: Wrap MNIST tensors in DataLoader objects for batch training and evaluation\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "mnist_train_loader = DataLoader(TensorDataset(x_train_torch, y_train_torch), batch_size=64, shuffle=True)\n",
        "mnist_test_loader = DataLoader(TensorDataset(x_test_torch, y_test_torch), batch_size=64, shuffle=False)\n",
        "\n",
        "# Object Header\n",
        "# Purpose: Initialize ResNet-18 model for grayscale MNIST (1 input channel)\n",
        "model_mnist = ResNet18(num_classes=10, in_channels=1).to(device)\n",
        "\n",
        "# Object Header\n",
        "# Purpose: Define cross-entropy loss with label smoothing\n",
        "criterion_mnist = SmoothCrossEntropyLoss(smoothing=LABEL_SMOOTHING)\n",
        "\n",
        "# Object Header\n",
        "# Purpose: Configure SGD optimizer with momentum and weight decay\n",
        "optimizer_mnist = optim.SGD(model_mnist.parameters(), lr=LEARNING_RATE,\n",
        "                            momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "# Object Header\n",
        "# Purpose: Train ResNet-18 model on MNIST and store training history\n",
        "print(\"Training ResNet-18 on MNIST...\")\n",
        "model_mnist, best_acc_mnist, train_acc_mnist, val_acc_mnist, train_loss_mnist, val_loss_mnist = train_and_evaluate_model(\n",
        "    model_mnist, mnist_train_loader, mnist_test_loader,\n",
        "    EPOCHS_MNIST_RESNET, criterion_mnist, optimizer_mnist\n",
        ")\n",
        "\n",
        "# output best test accuracy from training\n",
        "print(f\"Best MNIST ResNet18 Test Accuracy: {best_acc_mnist:.4f}\")\n"
      ],
      "metadata": {
        "id": "x3Vj8yAdrlVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 20: Plot ResNet-18 MNIST Training Curves\n",
        "\n",
        "# Description:\n",
        "#   This cell visualizes the training and validation performance of the ResNet-18 model on MNIST.\n",
        "#   Accuracy is plotted on a standard scale, while loss is shown on a logarithmic scale to highlight small improvements.\n",
        "#   Input: Training/validation accuracy and loss lists\n",
        "#   Output: Accuracy and loss plots across all epochs\n",
        "\n",
        "# This cell plots the training and validation accuracy/loss curves for ResNet-18 on MNIST.\n",
        "# Log-scale is used for the loss plot to emphasize changes during training.\n",
        "\n",
        "# define x-axis range based on number of epochs completed\n",
        "epochs_range = range(1, len(train_acc_mnist) + 1)\n",
        "\n",
        "# plot training and validation accuracy\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(epochs_range, train_acc_mnist, label='Train Accuracy')\n",
        "plt.plot(epochs_range, val_acc_mnist, label='Validation Accuracy')\n",
        "plt.title(\"MNIST ResNet-18 Accuracy\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# plot training and validation loss on a logarithmic y-axis\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(epochs_range, train_loss_mnist, label='Train Loss')\n",
        "plt.plot(epochs_range, val_loss_mnist, label='Validation Loss')\n",
        "plt.yscale(\"log\")\n",
        "plt.title(\"MNIST ResNet-18 Loss (Log Scale)\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "SQ1XuoPLrlN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 21: Evaluate ResNet-18 on MNIST\n",
        "\n",
        "# Description:\n",
        "#   This cell evaluates a trained ResNet-18 model on the MNIST test dataset using black-box testing.\n",
        "#   It computes overall accuracy, displays the confusion matrix, and visualizes correct and incorrect predictions.\n",
        "#   Input: Trained model, test DataLoader, test dataset tensors\n",
        "#   Output: Accuracy score, confusion matrix heatmap, and sample prediction grid\n",
        "\n",
        "# This cell evaluates the ResNet-18 model on the MNIST test set.\n",
        "# It outputs the test accuracy, plots the confusion matrix, and displays correct vs. incorrect predictions.\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "# set model to evaluation mode\n",
        "model_mnist.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "# run predictions across entire test set\n",
        "with torch.no_grad():\n",
        "    for images, labels in mnist_test_loader:\n",
        "        images = images.to(device)\n",
        "        outputs = model_mnist(images)\n",
        "        preds = outputs.argmax(dim=1).cpu().numpy()  # convert logits to predicted classes\n",
        "        all_preds.extend(preds)\n",
        "        all_labels.extend(labels.numpy())\n",
        "\n",
        "# compute confusion matrix and overall test accuracy\n",
        "conf_mat = confusion_matrix(all_labels, all_preds)\n",
        "accuracy = np.mean(np.array(all_preds) == np.array(all_labels))\n",
        "print(f\"MNIST ResNet-18 Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# plot confusion matrix heatmap\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
        "plt.title(\"MNIST ResNet-18 Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# identify indices for correct and incorrect classifications\n",
        "all_preds = np.array(all_preds)\n",
        "all_labels = np.array(all_labels)\n",
        "correct_idx = np.where(all_preds == all_labels)[0]\n",
        "incorrect_idx = np.where(all_preds != all_labels)[0]\n",
        "\n",
        "# Function Header\n",
        "# Description:\n",
        "#   Displays sample correct and incorrect predictions from MNIST using ResNet-18.\n",
        "#   Highlights accurate predictions in green and misclassifications in red.\n",
        "# Parameters:\n",
        "#   - x_test_torch: full test image tensor (N, 1, 28, 28)\n",
        "#   - correct_idx, incorrect_idx: indices of correct and incorrect predictions\n",
        "#   - preds, labels: predicted and true labels\n",
        "# Returns: None (plots sample images with prediction labels)\n",
        "def show_resnet_preds(x_test_torch, correct_idx, incorrect_idx, preds, labels):\n",
        "    idx_correct = np.random.choice(correct_idx, size=4, replace=False)\n",
        "    idx_incorrect = np.random.choice(incorrect_idx, size=4, replace=False)\n",
        "    selected = np.concatenate([idx_correct, idx_incorrect])\n",
        "\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    for i, idx in enumerate(selected):\n",
        "        img = x_test_torch[idx].squeeze().numpy()\n",
        "        true = labels[idx]\n",
        "        pred = preds[idx]\n",
        "        color = 'green' if i < 4 else 'red'\n",
        "        plt.subplot(2, 4, i+1)\n",
        "        plt.imshow(img, cmap='gray')\n",
        "        plt.title(f\"T:{true}, P:{pred}\", color=color)\n",
        "        plt.axis('off')\n",
        "    plt.suptitle(\"MNIST ResNet18 Predictions: Correct (green) vs Incorrect (red)\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# call visualization function to show prediction examples\n",
        "show_resnet_preds(x_test_torch, correct_idx, incorrect_idx, all_preds, all_labels)\n"
      ],
      "metadata": {
        "id": "6_p61tAesXu5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 22: Train ResNet-18 on CIFAR-10\n",
        "\n",
        "# Description:\n",
        "#   This cell trains a ResNet-18 model on the CIFAR-10 dataset using PyTorch.\n",
        "#   It uses label smoothing, SGD optimizer with momentum, and tracks model performance over 50 epochs.\n",
        "#   Input: CIFAR-10 training and test sets\n",
        "#   Output: Trained model, performance metrics, and best test accuracy\n",
        "\n",
        "# This cell trains a ResNet-18 model on the CIFAR-10 dataset using PyTorch.\n",
        "# It uses SGD optimizer, label smoothing, and tracks accuracy/loss for evaluation.\n",
        "\n",
        "# setup: training parameters\n",
        "EPOCHS_CIFAR_RESNET = 50\n",
        "LEARNING_RATE = 0.1\n",
        "MOMENTUM = 0.9\n",
        "WEIGHT_DECAY = 5e-4\n",
        "LABEL_SMOOTHING = 0.1\n",
        "\n",
        "# reshape CIFAR-10 data to PyTorch format (N, C, H, W)\n",
        "x_train_cifar_torch = torch.tensor(x_train_cifar.transpose(0, 3, 1, 2), dtype=torch.float32)\n",
        "y_train_cifar_torch = torch.tensor(y_train_cifar, dtype=torch.long)\n",
        "x_test_cifar_torch = torch.tensor(x_test_cifar.transpose(0, 3, 1, 2), dtype=torch.float32)\n",
        "y_test_cifar_torch = torch.tensor(y_test_cifar, dtype=torch.long)\n",
        "\n",
        "# create DataLoader objects for training and evaluation\n",
        "cifar_train_loader = DataLoader(TensorDataset(x_train_cifar_torch, y_train_cifar_torch), batch_size=64, shuffle=True)\n",
        "cifar_test_loader = DataLoader(TensorDataset(x_test_cifar_torch, y_test_cifar_torch), batch_size=64, shuffle=False)\n",
        "\n",
        "# initialize ResNet-18 for CIFAR-10 (3-channel RGB input)\n",
        "model_cifar = ResNet18(num_classes=10, in_channels=3).to(device)\n",
        "\n",
        "# define label smoothing loss\n",
        "criterion_cifar = SmoothCrossEntropyLoss(smoothing=LABEL_SMOOTHING)\n",
        "\n",
        "# configure SGD optimizer with weight decay and momentum\n",
        "optimizer_cifar = optim.SGD(model_cifar.parameters(), lr=LEARNING_RATE,\n",
        "                            momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "# train the model and store training history\n",
        "print(\"Training ResNet-18 on CIFAR-10...\")\n",
        "model_cifar, best_acc_cifar, train_acc_cifar, val_acc_cifar, train_loss_cifar, val_loss_cifar = \\\n",
        "    train_and_evaluate_model(model_cifar, cifar_train_loader, cifar_test_loader,\n",
        "                             EPOCHS_CIFAR_RESNET, criterion_cifar, optimizer_cifar)\n",
        "\n",
        "# display best validation accuracy\n",
        "print(f\"Best CIFAR-10 ResNet18 Test Accuracy: {best_acc_cifar:.4f}\")\n"
      ],
      "metadata": {
        "id": "yAN7PBAWsfeo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 23: CIFAR-10 ResNet-18 Training Curves\n",
        "\n",
        "# Description:\n",
        "#   This cell plots the training and validation accuracy and loss curves for the ResNet-18 model trained on CIFAR-10.\n",
        "#   Accuracy is plotted on a linear scale, while loss is shown on a logarithmic scale to highlight convergence trends.\n",
        "#   Input: Lists of accuracy and loss over epochs\n",
        "#   Output: Two plots: accuracy curve and log-scaled loss curve\n",
        "\n",
        "# Cell 23: CIFAR-10 ResNet-18 Training Curves\n",
        "# This cell visualizes the training and validation accuracy and loss for the ResNet-18 model trained on CIFAR-10.\n",
        "# Log-scale is used for the loss plot to highlight convergence behavior.\n",
        "\n",
        "# define x-axis range based on number of epochs completed\n",
        "epochs_range = range(1, len(train_acc_cifar) + 1)\n",
        "\n",
        "# plot training and validation accuracy curves\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(epochs_range, train_acc_cifar, label='Train Accuracy')\n",
        "plt.plot(epochs_range, val_acc_cifar, label='Validation Accuracy')\n",
        "plt.title(\"CIFAR-10 ResNet-18 Accuracy\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# plot training and validation loss curves with log-scaled y-axis\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(epochs_range, train_loss_cifar, label='Train Loss')\n",
        "plt.plot(epochs_range, val_loss_cifar, label='Validation Loss')\n",
        "plt.yscale(\"log\")\n",
        "plt.title(\"CIFAR-10 ResNet-18 Loss (Log Scale)\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7-BcCzIBsfcb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 24: Evaluate ResNet-18 on CIFAR-10\n",
        "\n",
        "# Description:\n",
        "#   This cell evaluates the trained ResNet-18 model on the CIFAR-10 dataset using black-box testing.\n",
        "#   It computes and displays the overall test accuracy and a confusion matrix,\n",
        "#   and visualizes example predictions, distinguishing between correct and incorrect outputs.\n",
        "#   Input: Trained model, CIFAR-10 test data\n",
        "#   Output: Accuracy score, confusion matrix plot, and sample image predictions\n",
        "\n",
        "# This cell evaluates the trained ResNet-18 model on the CIFAR-10 dataset.\n",
        "# It visualizes the confusion matrix and shows sample predictions (correct/incorrect).\n",
        "\n",
        "# set model to evaluation mode\n",
        "model_cifar.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "# generate predictions on entire test set\n",
        "with torch.no_grad():\n",
        "    for images, labels in cifar_test_loader:\n",
        "        images = images.to(device)\n",
        "        outputs = model_cifar(images)\n",
        "        preds = outputs.argmax(dim=1).cpu().numpy()  # get class with highest probability\n",
        "        all_preds.extend(preds)\n",
        "        all_labels.extend(labels.numpy())\n",
        "\n",
        "# compute confusion matrix and accuracy\n",
        "cifar_conf = confusion_matrix(all_labels, all_preds)\n",
        "accuracy = np.mean(np.array(all_preds) == np.array(all_labels))\n",
        "print(f\"CIFAR-10 ResNet-18 Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# plot confusion matrix with class labels\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cifar_conf, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=cifar10_classes, yticklabels=cifar10_classes)\n",
        "plt.title(\"CIFAR-10 ResNet-18 Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# convert predictions and labels to NumPy arrays for indexing\n",
        "all_preds = np.array(all_preds)\n",
        "all_labels = np.array(all_labels)\n",
        "correct_idx = np.where(all_preds == all_labels)[0]\n",
        "incorrect_idx = np.where(all_preds != all_labels)[0]\n",
        "\n",
        "# Function Header\n",
        "# Description:\n",
        "#   Displays a sample of correct and incorrect CIFAR-10 predictions made by ResNet-18.\n",
        "#   Correct predictions are shown in green; incorrect ones in red.\n",
        "# Parameters:\n",
        "#   - x_data: original input images (N, H, W, C)\n",
        "#   - correct_idx, incorrect_idx: arrays of correct/incorrect sample indices\n",
        "#   - preds: predicted class indices\n",
        "#   - labels: true class indices\n",
        "#   - class_names: list of class name strings for CIFAR-10\n",
        "# Returns: None (shows plot)\n",
        "def show_resnet_cifar_preds(x_data, correct_idx, incorrect_idx, preds, labels, class_names):\n",
        "    idx_correct = np.random.choice(correct_idx, size=4, replace=False)\n",
        "    idx_incorrect = np.random.choice(incorrect_idx, size=4, replace=False)\n",
        "    selected = np.concatenate([idx_correct, idx_incorrect])\n",
        "\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    for i, idx in enumerate(selected):\n",
        "        img = x_data[idx]\n",
        "        true = class_names[labels[idx]]\n",
        "        pred = class_names[preds[idx]]\n",
        "        color = 'green' if i < 4 else 'red'\n",
        "        plt.subplot(2, 4, i + 1)\n",
        "        plt.imshow(img)\n",
        "        plt.title(f\"T: {true}\\nP: {pred}\", color=color, fontsize=9)\n",
        "        plt.axis('off')\n",
        "    plt.suptitle(\"CIFAR-10 ResNet-18 Predictions: Correct (green) vs Incorrect (red)\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# call function to visualize correct and incorrect predictions\n",
        "show_resnet_cifar_preds(x_test_cifar, correct_idx, incorrect_idx, all_preds, all_labels, cifar10_classes)\n"
      ],
      "metadata": {
        "id": "PfGospMXsfZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 25: Train ResNet-18 No-Skip on CIFAR-10\n",
        "\n",
        "# Description:\n",
        "#   This cell defines and trains a variant of ResNet-18 with no skip connections for ablation testing on CIFAR-10.\n",
        "#   The goal is to compare performance against the standard ResNet-18 with residual shortcuts.\n",
        "#   Input: CIFAR-10 dataset, training hyperparameters\n",
        "#   Output: Trained no-skip model, performance history, and best test accuracy\n",
        "\n",
        "# This cell trains a modified ResNet-18 model with no skip connections (ResidualBlockNoSkip) for ablation testing.\n",
        "# The architecture is otherwise similar, and training follows the same procedure as standard ResNet.\n",
        "\n",
        "# training settings\n",
        "EPOCHS_NOSKIP = 50\n",
        "\n",
        "# User-Defined ADT Header\n",
        "# ADT Name: ResidualBlockNoSkip\n",
        "# Description:\n",
        "#   A basic residual-style convolutional block with no identity/skip connection.\n",
        "#   Used in constructing no-skip versions of ResNet for architecture ablation.\n",
        "class ResidualBlockNoSkip(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(ResidualBlockNoSkip, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.relu(self.bn2(self.conv2(out)))\n",
        "        return out\n",
        "\n",
        "# User-Defined ADT Header\n",
        "# ADT Name: ResNet18NoSkip\n",
        "# Description:\n",
        "#   A ResNet-like architecture composed of ResidualBlockNoSkip blocks (no skip connections).\n",
        "#   Used to compare performance with the standard ResNet-18 on CIFAR-10.\n",
        "class ResNet18NoSkip(nn.Module):\n",
        "    def __init__(self, num_classes=10, in_channels=3):\n",
        "        super(ResNet18NoSkip, self).__init__()\n",
        "        self.in_channels = 64\n",
        "        self.conv1 = nn.Conv2d(in_channels, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.layer1 = self._make_layer(64, 2, stride=1)\n",
        "        self.layer2 = self._make_layer(128, 2, stride=2)\n",
        "        self.layer3 = self._make_layer(256, 2, stride=2)\n",
        "        self.layer4 = self._make_layer(512, 2, stride=2)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512, num_classes)\n",
        "\n",
        "    def _make_layer(self, out_channels, blocks, stride):\n",
        "        layers = []\n",
        "        for i in range(blocks):\n",
        "            s = stride if i == 0 else 1\n",
        "            layers.append(ResidualBlockNoSkip(self.in_channels, out_channels, s))\n",
        "            self.in_channels = out_channels\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = self.avgpool(out)\n",
        "        out = torch.flatten(out, 1)\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "# Object Header\n",
        "# Purpose: Instantiate and train ResNet-18 without skip connections on CIFAR-10\n",
        "model_noskip = ResNet18NoSkip(num_classes=10, in_channels=3).to(device)\n",
        "criterion_noskip = SmoothCrossEntropyLoss(smoothing=LABEL_SMOOTHING)\n",
        "optimizer_noskip = optim.SGD(model_noskip.parameters(), lr=LEARNING_RATE,\n",
        "                             momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "print(\"Training ResNet-18 No-Skip on CIFAR-10...\")\n",
        "model_noskip, best_acc_noskip, train_acc_noskip, val_acc_noskip, train_loss_noskip, val_loss_noskip = \\\n",
        "    train_and_evaluate_model(model_noskip, cifar_train_loader, cifar_test_loader,\n",
        "                             EPOCHS_NOSKIP, criterion_noskip, optimizer_noskip)\n",
        "\n",
        "print(f\"Best Test Accuracy (No-Skip): {best_acc_noskip:.4f}\")\n"
      ],
      "metadata": {
        "id": "asulQOgDsfXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 26: No-Skip ResNet Training Curves (CIFAR-10)\n",
        "\n",
        "# Description:\n",
        "#   This cell visualizes the training and validation performance of the no-skip ResNet-18 model on CIFAR-10.\n",
        "#   It includes an accuracy curve and a log-scaled loss curve for ablation comparison with the standard ResNet.\n",
        "#   Input: Accuracy and loss history from no-skip model training\n",
        "#   Output: Accuracy plot and loss plot (log scale)\n",
        "\n",
        "# This cell plots the training and validation curves for ResNet-18 without skip connections.\n",
        "# Curves include standard accuracy and log-scaled loss to support ablation comparison.\n",
        "\n",
        "# define epoch range for x-axis\n",
        "epochs_range = range(1, len(train_acc_noskip) + 1)\n",
        "\n",
        "# plot training and validation accuracy\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(epochs_range, train_acc_noskip, label=\"Train Accuracy\")\n",
        "plt.plot(epochs_range, val_acc_noskip, label=\"Validation Accuracy\")\n",
        "plt.title(\"CIFAR-10 ResNet-18 No-Skip Accuracy\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# plot training and validation loss (log-scaled y-axis)\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(epochs_range, train_loss_noskip, label=\"Train Loss\")\n",
        "plt.plot(epochs_range, val_loss_noskip, label=\"Validation Loss\")\n",
        "plt.yscale(\"log\")\n",
        "plt.title(\"CIFAR-10 ResNet-18 No-Skip Loss (Log Scale)\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "cec_f4VMsfUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 27: Train Baseline CNN on CIFAR-10\n",
        "\n",
        "# Description:\n",
        "#   This cell builds and trains a baseline convolutional neural network (CNN) on the CIFAR-10 dataset.\n",
        "#   The model uses stacked convolutional layers without any skip/residual connections,\n",
        "#   providing a control architecture for ablation comparisons against ResNet variants.\n",
        "#   Input: CIFAR-10 dataset, training hyperparameters\n",
        "#   Output: Trained model, accuracy/loss history, and best validation accuracy\n",
        "\n",
        "# This cell builds and trains a basic CNN without residual connections, useful for ablation comparisons.\n",
        "\n",
        "# User-Defined ADT Header\n",
        "# ADT Name: BaselineCNN\n",
        "# Description:\n",
        "#   A simple convolutional neural network architecture using 4 convolutional layers,\n",
        "#   followed by global average pooling and a fully connected classification head.\n",
        "#   No skip connections or residual blocks are used.\n",
        "class BaselineCNN(nn.Module):\n",
        "    def __init__(self, num_classes=10, in_channels=3):\n",
        "        super(BaselineCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, 64, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
        "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512, num_classes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = self.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = self.relu(self.conv3(x))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = self.relu(self.conv4(x))\n",
        "        x = self.pool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Object Header\n",
        "# Purpose: Instantiate and train the baseline CNN on CIFAR-10\n",
        "model_cnn = BaselineCNN().to(device)\n",
        "\n",
        "# Object Header\n",
        "# Purpose: Define loss function with label smoothing for better generalization\n",
        "criterion_cnn = SmoothCrossEntropyLoss(smoothing=LABEL_SMOOTHING)\n",
        "\n",
        "# Object Header\n",
        "# Purpose: Use SGD optimizer with momentum and weight decay for training\n",
        "optimizer_cnn = optim.SGD(model_cnn.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "# Train the baseline CNN model and record its performance\n",
        "print(\"Training Baseline CNN on CIFAR-10...\")\n",
        "model_cnn, best_acc_cnn, train_acc_cnn, val_acc_cnn, train_loss_cnn, val_loss_cnn = \\\n",
        "    train_and_evaluate_model(model_cnn, cifar_train_loader, cifar_test_loader,\n",
        "                             EPOCHS_NOSKIP, criterion_cnn, optimizer_cnn)\n",
        "\n",
        "# Output best performance on test set\n",
        "print(f\"Best Test Accuracy (Baseline CNN): {best_acc_cnn:.4f}\")\n"
      ],
      "metadata": {
        "id": "fWchqKwfsfRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 28: Evaluate Baseline CNN on CIFAR-10\n",
        "\n",
        "# Description:\n",
        "#   This cell evaluates the trained Baseline CNN model on the CIFAR-10 test dataset.\n",
        "#   It reports test accuracy, generates a confusion matrix for class-wise evaluation,\n",
        "#   and visualizes prediction examples with correct and incorrect classifications.\n",
        "#   Input: Trained BaselineCNN model, CIFAR-10 test data\n",
        "#   Output: Accuracy score, confusion matrix, and prediction visualization\n",
        "\n",
        "# This cell evaluates the baseline CNN model, including confusion matrix and prediction examples.\n",
        "\n",
        "# set model to evaluation mode\n",
        "model_cnn.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "# run predictions on full CIFAR-10 test set\n",
        "with torch.no_grad():\n",
        "    for images, labels in cifar_test_loader:\n",
        "        images = images.to(device)\n",
        "        outputs = model_cnn(images)\n",
        "        preds = outputs.argmax(dim=1).cpu().numpy()  # predicted class indices\n",
        "        all_preds.extend(preds)\n",
        "        all_labels.extend(labels.numpy())\n",
        "\n",
        "# compute and print test accuracy\n",
        "cnn_conf = confusion_matrix(all_labels, all_preds)\n",
        "print(f\"Baseline CNN Test Accuracy: {np.mean(np.array(all_preds) == np.array(all_labels)):.4f}\")\n",
        "\n",
        "# plot confusion matrix heatmap with CIFAR-10 class labels\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cnn_conf, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=cifar10_classes, yticklabels=cifar10_classes)\n",
        "plt.title(\"CIFAR-10 Baseline CNN Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# visualize example predictions using helper from Cell 24\n",
        "show_resnet_cifar_preds(\n",
        "    x_test_cifar,\n",
        "    np.where(np.array(all_preds) == np.array(all_labels))[0],  # correct indices\n",
        "    np.where(np.array(all_preds) != np.array(all_labels))[0],  # incorrect indices\n",
        "    all_preds,\n",
        "    all_labels,\n",
        "    cifar10_classes\n",
        ")\n"
      ],
      "metadata": {
        "id": "N1hFL7lfsfOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SECTION 5"
      ],
      "metadata": {
        "id": "hWDWdi-S1gE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 29: Print Model Summaries\n",
        "\n",
        "# Description:\n",
        "#   This cell prints model summaries using `torchsummary` for all trained networks in the project.\n",
        "#   It allows architectural comparison across ResNet-18 (with/without skips), baseline CNN, and MNIST/CIFAR variants.\n",
        "#   Input: Model instances (already trained)\n",
        "#   Output: Model layer breakdowns, parameter counts, output shapes\n",
        "\n",
        "# This cell summarizes all trained models using torchsummary for architecture comparison.\n",
        "\n",
        "from torchsummary import summary\n",
        "\n",
        "# print layer-wise structure and parameter counts for each model\n",
        "\n",
        "print(\"\\n--- ResNet-18 Summary (MNIST) ---\")\n",
        "summary(model_mnist, input_size=(1, 28, 28))\n",
        "\n",
        "print(\"\\n--- ResNet-18 Summary (CIFAR-10) ---\")\n",
        "summary(model_cifar, input_size=(3, 32, 32))\n",
        "\n",
        "print(\"\\n--- No-Skip ResNet Summary ---\")\n",
        "summary(model_noskip, input_size=(3, 32, 32))\n",
        "\n",
        "print(\"\\n--- Baseline CNN Summary ---\")\n",
        "summary(model_cnn, input_size=(3, 32, 32))\n"
      ],
      "metadata": {
        "id": "lBGvK1wZsfMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 30: Visualize ResNet and Baseline Weights\n",
        "\n",
        "# Description:\n",
        "#   This cell performs white-box testing visualizations for trained PyTorch models.\n",
        "#   It displays the learned filters from the first convolutional layer (conv1)\n",
        "#   and visualizes the dense (fully connected) output layer's weight matrix using heatmaps.\n",
        "#   Input: Trained models (ResNet and BaselineCNN)\n",
        "#   Output: Grid of filter visualizations and heatmaps of fully connected layer weights\n",
        "\n",
        "# This cell visualizes conv1 filters and dense layer weights from trained models.\n",
        "\n",
        "# Function Header\n",
        "# Description:\n",
        "#   Visualizes the first convolutional layer filters of a given model.\n",
        "#   Filters are normalized to [0, 1] and shown as individual images.\n",
        "# Parameters:\n",
        "#   - model: a PyTorch CNN or ResNet model\n",
        "#   - title: title prefix for the plot\n",
        "# Returns: None (displays matplotlib figure)\n",
        "def show_conv_weights(model, title):\n",
        "    conv_weights = model.conv1.weight.data.clone().cpu()\n",
        "    conv_weights = (conv_weights - conv_weights.min()) / (conv_weights.max() - conv_weights.min())\n",
        "\n",
        "    fig, axes = plt.subplots(1, min(8, conv_weights.shape[0]), figsize=(12, 3))\n",
        "    fig.suptitle(f\"{title} - First Conv Layer Filters\", fontsize=14)\n",
        "\n",
        "    for i in range(min(8, conv_weights.shape[0])):\n",
        "        img = conv_weights[i].squeeze()\n",
        "        axes[i].imshow(img.permute(1, 2, 0) if img.ndim == 3 else img, cmap='gray')\n",
        "        axes[i].axis(\"off\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Function Header\n",
        "# Description:\n",
        "#   Visualizes the fully connected (FC) output layer weights as a 2D heatmap.\n",
        "# Parameters:\n",
        "#   - model: a PyTorch model with a .fc linear output layer\n",
        "#   - title: title prefix for the heatmap\n",
        "# Returns: None (displays matplotlib heatmap)\n",
        "def show_fc_weights(model, title):\n",
        "    fc_weights = model.fc.weight.data.clone().cpu()\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    sns.heatmap(fc_weights, cmap=\"viridis\")\n",
        "    plt.title(f\"{title} - FC Layer Weight Matrix ({fc_weights.shape[0]} classes × {fc_weights.shape[1]} features)\")\n",
        "    plt.xlabel(\"Features\")\n",
        "    plt.ylabel(\"Classes\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# visualize weights for all trained PyTorch models\n",
        "show_conv_weights(model_mnist, \"MNIST ResNet-18\")\n",
        "show_fc_weights(model_mnist, \"MNIST ResNet-18\")\n",
        "\n",
        "show_conv_weights(model_cifar, \"CIFAR-10 ResNet-18\")\n",
        "show_fc_weights(model_cifar, \"CIFAR-10 ResNet-18\")\n",
        "\n",
        "show_conv_weights(model_noskip, \"CIFAR-10 ResNet-18 No-Skip\")\n",
        "show_fc_weights(model_noskip, \"CIFAR-10 ResNet-18 No-Skip\")\n",
        "\n",
        "show_conv_weights(model_cnn, \"CIFAR-10 Baseline CNN\")\n",
        "show_fc_weights(model_cnn, \"CIFAR-10 Baseline CNN\")\n"
      ],
      "metadata": {
        "id": "7UZYuxpi0X3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 31: Black-Box Testing for Final Model Performance\n",
        "\n",
        "# Description:\n",
        "#   This cell performs formal black-box testing on each trained model using unseen test data.\n",
        "#   Models are treated as black boxes — their outputs are assessed solely based on input-output behavior.\n",
        "#   Evaluation includes a detailed classification report with precision, recall, F1-score, and support for each class.\n",
        "#   Input: Trained model, test data tensors\n",
        "#   Output: Formatted classification reports showing generalization performance\n",
        "\n",
        "# This cell performs explicit black-box testing on the final trained models by evaluating their input–output behavior.\n",
        "# Models are evaluated on unseen test sets without accessing internal implementation or weights.\n",
        "# It confirms functional correctness and compares generalization across models.\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Function Header\n",
        "# Description:\n",
        "#   Runs black-box evaluation of a model by comparing predicted and true labels.\n",
        "#   Outputs a classification report summarizing key performance metrics.\n",
        "# Parameters:\n",
        "#   - model: a trained PyTorch model\n",
        "#   - x_test_tensor: input images tensor\n",
        "#   - y_test_tensor: ground truth labels tensor\n",
        "#   - dataset_name: string label for output report title\n",
        "# Returns: None (prints output)\n",
        "def black_box_test(model, x_test_tensor, y_test_tensor, dataset_name):\n",
        "    \"\"\"run black-box testing on test data and print evaluation summary\"\"\"\n",
        "    model.eval()\n",
        "    y_true = y_test_tensor.numpy()\n",
        "    y_pred = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, _ in DataLoader(TensorDataset(x_test_tensor, y_test_tensor), batch_size=64):\n",
        "            inputs = inputs.to(device)  # input already correctly shaped\n",
        "            outputs = model(inputs)\n",
        "            preds = outputs.argmax(dim=1).cpu().numpy()\n",
        "            y_pred.extend(preds)\n",
        "\n",
        "    print(f\"\\n─── BLACK BOX TEST REPORT ({dataset_name}) ───\")\n",
        "    print(classification_report(y_true, y_pred, digits=4))\n",
        "\n",
        "# Run black-box tests on all final models\n",
        "black_box_test(model_mnist, x_test_torch, y_test_torch, \"MNIST ResNet-18\")\n",
        "\n",
        "black_box_test(\n",
        "    model_cifar,\n",
        "    torch.tensor(x_test_cifar.transpose(0, 3, 1, 2)).float(),\n",
        "    torch.tensor(y_test_cifar),\n",
        "    \"CIFAR-10 ResNet-18\"\n",
        ")\n",
        "\n",
        "black_box_test(\n",
        "    model_noskip,\n",
        "    torch.tensor(x_test_cifar.transpose(0, 3, 1, 2)).float(),\n",
        "    torch.tensor(y_test_cifar),\n",
        "    \"CIFAR-10 No-Skip ResNet\"\n",
        ")\n",
        "\n",
        "black_box_test(\n",
        "    model_cnn,\n",
        "    torch.tensor(x_test_cifar.transpose(0, 3, 1, 2)).float(),\n",
        "    torch.tensor(y_test_cifar),\n",
        "    \"CIFAR-10 Baseline CNN\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "1gQ10H680qu4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}